version: '3.9'
name: gpu-demo
services:
  mistral:
    restart: unless-stopped
    image: ghcr.io/mistralai/mistral-src/vllm:latest
    ports:
      - mode: host
        target: 8000
    command: ["--host", "0.0.0.0", "--model", "TheBloke/Mistral-7B-Instruct-v0.2-AWQ", "--quantization", "awq", "--dtype", "auto", "--tensor-parallel-size", "1", "--gpu-memory-utilization", ".95", "--max-model-len", "8000"]
    deploy:
      resources:
        reservations:
          cpus: '2.0'
          memory: 8192M
          devices:
            - capabilities: ["gpu"]
    healthcheck:
      test: ["CMD", "curl", "http://localhost:8000/v1/models"]
      interval: 5m
      timeout: 30s
      retries: 10
    environment:
      - HF_TOKEN
  ui:
    restart: unless-stopped
    build:
      context: ui
      dockerfile: Dockerfile
    ports:
      - mode: ingress
        target: 3000
    deploy:
      resources:
        reservations:
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:3000"]
      interval: 10s
      timeout: 2s
      retries: 10
    environment:
      - OPENAI_BASE_URL=http://mistral:8000/v1/
